{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Decision_Trees_Trained Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPC9lXQk9Z21GpwHZJPfEie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9edab0aa01a4a0e9def2e0323ee2f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8454019ee8714095a5b147ccf4f576b1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bfd8960332224f4d8a37c252ba157e6b",
              "IPY_MODEL_3042aa7a2322417a96f5a4d70edfe7a8",
              "IPY_MODEL_f5fb7dc27683460493c4e8e830dafdef",
              "IPY_MODEL_9030312d0de84fa38e635fccb2c417fb",
              "IPY_MODEL_48f5de84c4a4451494daa184ae6de3e6"
            ]
          }
        },
        "8454019ee8714095a5b147ccf4f576b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "column",
            "width": "50%",
            "min_width": null,
            "border": null,
            "align_items": "center",
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "flex",
            "left": null
          }
        },
        "bfd8960332224f4d8a37c252ba157e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36674f27ddcd4821a32b14ae7fee9931",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "<center>\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svg alt='Hugging Face'>\n<br>\nCopy a token from <a href=\"https://huggingface.co/settings/token\" target=\"_blank\">your Hugging Face tokens page</a> and paste it below.\n<br>\nImmediately click login after copying your token or it might be stored in plain text in this notebook file.\n</center>",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21fdd5ba153b4ac89a063a62b60041ff"
          }
        },
        "3042aa7a2322417a96f5a4d70edfe7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "PasswordView",
            "style": "IPY_MODEL_6b8c40f39e5e49cf831b7712ad106475",
            "_dom_classes": [],
            "description": "Token:",
            "_model_name": "PasswordModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4492f7791bf4c5dbdeba0ef9857254e"
          }
        },
        "f5fb7dc27683460493c4e8e830dafdef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_030808753c2c4f04b7f5122d57d4abeb",
            "_dom_classes": [],
            "description": "Login",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_4136d0c21f394960b8db1b245904e8f3",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "9030312d0de84fa38e635fccb2c417fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f839eeb866c43cc98d0dd844603e3ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated 'notebooks' token with 'write' access, that you can then easily reuse for all notebooks.\n<br>\n<i>Logging in with your username and password is deprecated and won't be possible anymore in the near future. You can still use them for now by clicking below.</i>\n</center>",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3250d0badbad46a1b1bddc7aae72d3f0"
          }
        },
        "48f5de84c4a4451494daa184ae6de3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_8057d50dd0ce4a8484ef87be373f212c",
            "_dom_classes": [],
            "description": "Use password",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_84fac5a143dd430a880a5dad8a2e5863",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "36674f27ddcd4821a32b14ae7fee9931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21fdd5ba153b4ac89a063a62b60041ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b8c40f39e5e49cf831b7712ad106475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4492f7791bf4c5dbdeba0ef9857254e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "030808753c2c4f04b7f5122d57d4abeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4136d0c21f394960b8db1b245904e8f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f839eeb866c43cc98d0dd844603e3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3250d0badbad46a1b1bddc7aae72d3f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8057d50dd0ce4a8484ef87be373f212c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84fac5a143dd430a880a5dad8a2e5863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdubon/TF-Decision-Forest/blob/main/TF_Decision_Trees_Trained_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification with TF Decision Trees\n",
        "Source code from https://keras.io/examples/structured_data/classification_with_tfdf/"
      ],
      "metadata": {
        "id": "yF3bjhkdylqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "va3EN2aRGAj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow_decision_forests"
      ],
      "metadata": {
        "id": "ZWA4QL6uy0r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.7.0"
      ],
      "metadata": {
        "id": "4bKMv9ftXH43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db4ce5c-afc8-469e-a705-84ee63ba8d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.7.0 in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.43.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (13.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.10.0.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.7.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.23.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipykernel==4.10"
      ],
      "metadata": {
        "id": "STU_6dlAX4v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y git-lfs"
      ],
      "metadata": {
        "id": "j1T0CXtN6aZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import push_to_hub_keras\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "d9edab0aa01a4a0e9def2e0323ee2f5d",
            "8454019ee8714095a5b147ccf4f576b1",
            "bfd8960332224f4d8a37c252ba157e6b",
            "3042aa7a2322417a96f5a4d70edfe7a8",
            "f5fb7dc27683460493c4e8e830dafdef",
            "9030312d0de84fa38e635fccb2c417fb",
            "48f5de84c4a4451494daa184ae6de3e6",
            "36674f27ddcd4821a32b14ae7fee9931",
            "21fdd5ba153b4ac89a063a62b60041ff",
            "6b8c40f39e5e49cf831b7712ad106475",
            "e4492f7791bf4c5dbdeba0ef9857254e",
            "030808753c2c4f04b7f5122d57d4abeb",
            "4136d0c21f394960b8db1b245904e8f3",
            "7f839eeb866c43cc98d0dd844603e3ca",
            "3250d0badbad46a1b1bddc7aae72d3f0",
            "8057d50dd0ce4a8484ef87be373f212c",
            "84fac5a143dd430a880a5dad8a2e5863"
          ]
        },
        "id": "aUbSq698F6nB",
        "outputId": "8b8628d5-03c8-4d8f-e452-672085024786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
            "\n",
            "git config --global credential.helper store\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_decision_forests as tfdf\n"
      ],
      "metadata": {
        "id": "l2rrOS095XCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418235d7-8b3d-4950-bdad-2a951a635a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TF Parameter Server distributed training not available (this is expected for the pre-build release).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income\"\n",
        "input_column_header = \"income_level\"\n"
      ],
      "metadata": {
        "id": "l8mDDcjSiRXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "\n",
        "BASE_PATH = input_path\n",
        "CSV_HEADER = [ l.decode(\"utf-8\").split(\":\")[0].replace(\" \", \"_\")\n",
        "  for l in urllib.request.urlopen(f\"{BASE_PATH}.names\")\n",
        "  if not l.startswith(b\"|\")][2:]\n",
        "\n",
        "CSV_HEADER.append(input_column_header)\n",
        "\n",
        "train_data = pd.read_csv(f\"{BASE_PATH}.data.gz\", header=None, names=CSV_HEADER)\n",
        "test_data = pd.read_csv(f\"{BASE_PATH}.test.gz\", header=None, names=CSV_HEADER)"
      ],
      "metadata": {
        "id": "dERuHCWf5XR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.head)"
      ],
      "metadata": {
        "id": "3C7PqSjpi_rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert from string to integers\n",
        "target_labels = [\" - 50000.\", \" 50000+.\"]\n",
        "train_data[input_column_header] = train_data[input_column_header].map(target_labels.index)\n",
        "test_data[input_column_header] = test_data[input_column_header].map(target_labels.index)"
      ],
      "metadata": {
        "id": "-s7hgWhv5Xi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observe shape of training and test data\n",
        "print(f\"Train data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(train_data.head().T)"
      ],
      "metadata": {
        "id": "wlxsGTnVVIEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define metadata\n",
        "\n",
        "# Target column name.\n",
        "TARGET_COLUMN_NAME = \"income_level\"\n",
        "# Weight column name.\n",
        "WEIGHT_COLUMN_NAME = \"instance_weight\"\n",
        "# Numeric feature names.\n",
        "NUMERIC_FEATURE_NAMES = [\n",
        "    \"age\",\n",
        "    \"wage_per_hour\",\n",
        "    \"capital_gains\",\n",
        "    \"capital_losses\",\n",
        "    \"dividends_from_stocks\",\n",
        "    \"num_persons_worked_for_employer\",\n",
        "    \"weeks_worked_in_year\",\n",
        "]\n",
        "\n",
        "# Categorical features and their vocabulary lists.\n",
        "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
        "    feature_name: sorted(\n",
        "        [str(value) for value in list(train_data[feature_name].unique())]\n",
        "    )\n",
        "    for feature_name in CSV_HEADER\n",
        "    if feature_name\n",
        "    not in list(NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME, TARGET_COLUMN_NAME])\n",
        "}\n",
        "# All features names.\n",
        "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + list(\n",
        "    CATEGORICAL_FEATURES_WITH_VOCABULARY.keys()\n",
        ")"
      ],
      "metadata": {
        "id": "y0aYaRRjVIQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure hyperparameters for the tree model."
      ],
      "metadata": {
        "id": "InpzY04D1CZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROWING_STRATEGY = \"BEST_FIRST_GLOBAL\"\n",
        "NUM_TREES = 250\n",
        "MIN_EXAMPLES = 6\n",
        "MAX_DEPTH = 5\n",
        "SUBSAMPLE = 0.65\n",
        "SAMPLING_METHOD = \"RANDOM\"\n",
        "VALIDATION_RATIO = 0.1"
      ],
      "metadata": {
        "id": "kz0XxQed1dW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement training & evaluation procedure\n",
        "def prepare_sample(features, target, weight):\n",
        "    for feature_name in features:\n",
        "        if feature_name in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
        "            if features[feature_name].dtype != tf.dtypes.string:\n",
        "                # Convert categorical feature values to string.\n",
        "                features[feature_name] = tf.strings.as_string(features[feature_name])\n",
        "    return features, target, weight\n",
        "\n",
        "\n",
        "def run_experiment(model, train_data, test_data, num_epochs=1, batch_size=None):\n",
        "\n",
        "    train_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
        "        train_data, label=TARGET_COLUMN_NAME, weight=WEIGHT_COLUMN_NAME\n",
        "    ).map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    test_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
        "        test_data, label=TARGET_COLUMN_NAME, weight=WEIGHT_COLUMN_NAME\n",
        "    ).map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    model.fit(train_dataset, epochs=num_epochs, batch_size=batch_size)\n",
        "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "    push_to_hub = True\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    "
      ],
      "metadata": {
        "id": "c4VhjQKBWmW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create model inputs\n",
        "\n",
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.float32\n",
        "            )\n",
        "        else:\n",
        "            inputs[feature_name] = layers.Input(\n",
        "                name=feature_name, shape=(), dtype=tf.string\n",
        "            )\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "yIJ7qRNuXhIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Forests for Target Encoding\n",
        " \n",
        "\n",
        "*   Convert categorical features using numerical encoding\n",
        "*   Extract positive_frequency, negative_frequency, positive_probability\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1IaVLtS1wJWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement Binary Target Encoder\n",
        "class BinaryTargetEncoding(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def adapt(self, data):\n",
        "        # data is expected to be an integer numpy array to a Tensor shape [num_exmples, 2].\n",
        "        # This contains feature values for a given feature in the dataset, and target values.\n",
        "\n",
        "        # Convert the data to a tensor.\n",
        "        data = tf.convert_to_tensor(data)\n",
        "        # Separate the feature values and target values\n",
        "        feature_values = tf.cast(data[:, 0], tf.dtypes.int64)\n",
        "        target_values = tf.cast(data[:, 1], tf.dtypes.bool)\n",
        "\n",
        "        print(\"Target encoding: Computing unique feature values...\")\n",
        "        # Get feature vocabulary.\n",
        "        unique_feature_values = tf.sort(tf.unique(feature_values).y)\n",
        "\n",
        "        print(\n",
        "            \"Target encoding: Computing frequencies for feature values with positive targets...\"\n",
        "        )\n",
        "        # Filter the data where the target label is positive.\n",
        "        positive_indices = tf.where(condition=target_values)\n",
        "        postive_feature_values = tf.gather_nd(\n",
        "            params=feature_values, indices=positive_indices\n",
        "        )\n",
        "        # Compute how many times each feature value occurred with a positive target label.\n",
        "        positive_frequency = tf.math.unsorted_segment_sum(\n",
        "            data=tf.ones(\n",
        "                shape=(postive_feature_values.shape[0], 1), dtype=tf.dtypes.int32\n",
        "            ),\n",
        "            segment_ids=postive_feature_values,\n",
        "            num_segments=unique_feature_values.shape[0],\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            \"Target encoding: Computing frequencies for feature values with negative targets...\"\n",
        "        )\n",
        "        # Filter the data where the target label is negative.\n",
        "        negative_indices = tf.where(condition=tf.math.logical_not(target_values))\n",
        "        negative_feature_values = tf.gather_nd(\n",
        "            params=feature_values, indices=negative_indices\n",
        "        )\n",
        "        # Compute how many times each feature value occurred with a negative target label.\n",
        "        negative_frequency = tf.math.unsorted_segment_sum(\n",
        "            data=tf.ones(\n",
        "                shape=(negative_feature_values.shape[0], 1), dtype=tf.dtypes.int32\n",
        "            ),\n",
        "            segment_ids=negative_feature_values,\n",
        "            num_segments=unique_feature_values.shape[0],\n",
        "        )\n",
        "\n",
        "        print(\"Target encoding: Storing target encoding statistics...\")\n",
        "        self.positive_frequency_lookup = tf.constant(positive_frequency)\n",
        "        self.negative_frequency_lookup = tf.constant(negative_frequency)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.positive_frequency_lookup = None\n",
        "        self.negative_frequency_lookup = None\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs is expected to be an integer numpy array to a Tensor shape [num_exmples, 1].\n",
        "        # This includes the feature values for a given feature in the dataset.\n",
        "\n",
        "        # Raise an error if the target encoding statistics are not computed.\n",
        "        if (\n",
        "            self.positive_frequency_lookup == None\n",
        "            or self.negative_frequency_lookup == None\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"You need to call the adapt method to compute target encoding statistics.\"\n",
        "            )\n",
        "\n",
        "        # Convert the inputs to a tensor.\n",
        "        inputs = tf.convert_to_tensor(inputs)\n",
        "        # Cast the inputs int64 a tensor.\n",
        "        inputs = tf.cast(inputs, tf.dtypes.int64)\n",
        "        # Lookup positive frequencies for the input feature values.\n",
        "        positive_fequency = tf.cast(\n",
        "            tf.gather_nd(self.positive_frequency_lookup, inputs),\n",
        "            dtype=tf.dtypes.float32,\n",
        "        )\n",
        "        # Lookup negative frequencies for the input feature values.\n",
        "        negative_fequency = tf.cast(\n",
        "            tf.gather_nd(self.negative_frequency_lookup, inputs),\n",
        "            dtype=tf.dtypes.float32,\n",
        "        )\n",
        "        # Compute positive probability for the input feature values.\n",
        "        positive_probability = positive_fequency / (\n",
        "            positive_fequency + negative_fequency\n",
        "        )\n",
        "        # Concatenate and return the looked-up statistics.\n",
        "        return tf.concat(\n",
        "            [positive_fequency, negative_fequency, positive_probability], axis=1\n",
        "        )"
      ],
      "metadata": {
        "id": "Bm_hPVhMv4X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test binary target encoder\n",
        "data = tf.constant(\n",
        "    [\n",
        "        [0, 1],\n",
        "        [2, 0],\n",
        "        [0, 1],\n",
        "        [1, 1],\n",
        "        [1, 1],\n",
        "        [2, 0],\n",
        "        [1, 0],\n",
        "        [0, 1],\n",
        "        [2, 1],\n",
        "        [1, 0],\n",
        "        [0, 1],\n",
        "        [2, 0],\n",
        "        [0, 1],\n",
        "        [1, 1],\n",
        "        [1, 1],\n",
        "        [2, 0],\n",
        "        [1, 0],\n",
        "        [0, 1],\n",
        "        [2, 0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "binary_target_encoder = BinaryTargetEncoding()\n",
        "binary_target_encoder.adapt(data)\n",
        "print(binary_target_encoder([[0], [1], [2]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiFCFsEMy7QF",
        "outputId": "1d8f7ac9-8ae4-466d-9b4d-ca3a07209b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target encoding: Computing unique feature values...\n",
            "Target encoding: Computing frequencies for feature values with positive targets...\n",
            "Target encoding: Computing frequencies for feature values with negative targets...\n",
            "Target encoding: Storing target encoding statistics...\n",
            "tf.Tensor(\n",
            "[[6.         0.         1.        ]\n",
            " [4.         3.         0.5714286 ]\n",
            " [1.         5.         0.16666667]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement a feature encoding with target encoding\n",
        "\n",
        "def create_target_encoder():\n",
        "    inputs = create_model_inputs()\n",
        "    target_values = train_data[[TARGET_COLUMN_NAME]].to_numpy()\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
        "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "            # Create a lookup to convert string values to an integer indices.\n",
        "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
        "            lookup = layers.StringLookup(\n",
        "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
        "            )\n",
        "            # Convert the string input values into integer indices.\n",
        "            value_indices = lookup(inputs[feature_name])\n",
        "            # Prepare the data to adapt the target encoding.\n",
        "            print(\"### Adapting target encoding for:\", feature_name)\n",
        "            feature_values = train_data[[feature_name]].to_numpy().astype(str)\n",
        "            feature_value_indices = lookup(feature_values)\n",
        "            data = tf.concat([feature_value_indices, target_values], axis=1)\n",
        "            feature_encoder = BinaryTargetEncoding()\n",
        "            feature_encoder.adapt(data)\n",
        "            # Convert the feature value indices to target encoding representations.\n",
        "            encoded_feature = feature_encoder(tf.expand_dims(value_indices, -1))\n",
        "        else:\n",
        "            # Expand the dimensions of the numerical input feature and use it as-is.\n",
        "            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
        "        # Add the encoded feature to the list.\n",
        "        encoded_features.append(encoded_feature)\n",
        "    # Concatenate all the encoded features.\n",
        "    encoded_features = tf.concat(encoded_features, axis=1)\n",
        "    # Create and return a Keras model with encoded features as outputs.\n",
        "    return keras.Model(inputs=inputs, outputs=encoded_features)"
      ],
      "metadata": {
        "id": "ECHne8obzpDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Boosted Tree Model with preprocessor\n",
        "\n",
        "def create_gbt_with_preprocessor(preprocessor):\n",
        "\n",
        "    gbt_model = tfdf.keras.GradientBoostedTreesModel(\n",
        "        preprocessing=preprocessor,\n",
        "        growing_strategy=GROWING_STRATEGY,\n",
        "        num_trees=NUM_TREES,\n",
        "        max_depth=MAX_DEPTH,\n",
        "        min_examples=MIN_EXAMPLES,\n",
        "        subsample=SUBSAMPLE,\n",
        "        validation_ratio=VALIDATION_RATIO,\n",
        "        task=tfdf.keras.Task.CLASSIFICATION,\n",
        "    )\n",
        "\n",
        "    gbt_model.compile(metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
        "\n",
        "    return gbt_model"
      ],
      "metadata": {
        "id": "NUH27drXzzw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and evaluate the model\n",
        "\n",
        "gbt_model = create_gbt_with_preprocessor(create_target_encoder())\n",
        "run_experiment(gbt_model, train_data, test_data)"
      ],
      "metadata": {
        "id": "OUkPb8y50AKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Forests with trained embeddings\n",
        "\n",
        "1.   Train the embedding encoder with linear model with backprop\n",
        "2.   Use encoder to preprocess input features of GB Tree\n"
      ],
      "metadata": {
        "id": "Kxls-pzq0T0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement feature encoding with embeddings\n",
        "def create_embedding_encoder():\n",
        "    inputs = create_model_inputs()\n",
        "    encoded_features = []\n",
        "    for feature_name in inputs:\n",
        "        if feature_name in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
        "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
        "            # Create a lookup to convert string values to an integer indices.\n",
        "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
        "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
        "            lookup = layers.StringLookup(\n",
        "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
        "            )\n",
        "            # Convert the string input values into integer indices.\n",
        "            value_index = lookup(inputs[feature_name])\n",
        "            # Create an embedding layer with the specified dimensions\n",
        "            vocabulary_size = len(vocabulary)\n",
        "            embedding_size = int(math.sqrt(vocabulary_size))\n",
        "            feature_encoder = layers.Embedding(\n",
        "                input_dim=len(vocabulary), output_dim=embedding_size\n",
        "            )\n",
        "            # Convert the index values to embedding representations.\n",
        "            encoded_feature = feature_encoder(value_index)\n",
        "        else:\n",
        "            # Expand the dimensions of the numerical input feature and use it as-is.\n",
        "            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
        "        # Add the encoded feature to the list.\n",
        "        encoded_features.append(encoded_feature)\n",
        "    # Concatenate all the encoded features.\n",
        "    encoded_features = layers.concatenate(encoded_features, axis=1)\n",
        "    # Create and return a Keras model with encoded features as outputs.\n",
        "    return keras.Model(inputs=inputs, outputs=encoded_features)"
      ],
      "metadata": {
        "id": "6o1TZvoK0UPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build linear model to train embeddings\n",
        "def create_linear_model(encoder):\n",
        "    inputs = create_model_inputs()\n",
        "    embeddings = encoder(inputs)\n",
        "    linear_output = layers.Dense(units=1, activation=\"sigmoid\")(embeddings)\n",
        "\n",
        "    linear_model = keras.Model(inputs=inputs, outputs=linear_output)\n",
        "    linear_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[keras.metrics.BinaryAccuracy(\"accuracy\")],\n",
        "    )\n",
        "    return linear_model\n",
        "\n",
        "\n",
        "embedding_encoder = create_embedding_encoder()\n",
        "run_experiment(\n",
        "    create_linear_model(embedding_encoder),\n",
        "    train_data,\n",
        "    test_data,\n",
        "    num_epochs=3,\n",
        "    batch_size=256,\n",
        ")"
      ],
      "metadata": {
        "id": "7S3C_B3z3TYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate GB Tree model with embeddings"
      ],
      "metadata": {
        "id": "v4x6p-_r3bpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbt_model = create_gbt_with_preprocessor(embedding_encoder)\n",
        "run_experiment(gbt_model, train_data, test_data)"
      ],
      "metadata": {
        "id": "0b9vECd03i4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbt_model.get_weights()"
      ],
      "metadata": {
        "id": "KMnpKhWA4XJH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}